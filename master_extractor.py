import pandas as pd
import json
import re
from datetime import datetime, timedelta
from collections import Counter
import unicodedata

def clean_description(description):
    """Remove ruÃ­dos e informaÃ§Ãµes irrelevantes da descriÃ§Ã£o"""
    if not description or pd.isna(description):
        return ""
    
    desc = str(description)
    
    # Remove padrÃµes de ruÃ­do comuns
    noise_patterns = [
        r'candidatura\s*f[aÃ¡]cil',
        r'continuar\s*lendo',
        r'leia\s*mais',
        r'ver\s*mais',
        r'clique\s*aqui',
        r'saiba\s*mais',
        r'acesse\s*o\s*link',
        r'entre\s*em\s*contato',
        r'envie\s*seu\s*curr[Ã­i]culo',
        r'cadastre[\-\s]*se',
        r'inscreva[\-\s]*se',
        r'aplique[\-\s]*se',
        r'candidate[\-\s]*se',
        r'whatsapp\s*\d+',
        r'telefone\s*\d+',
        r'email\s*[^\s]+@[^\s]+',
        r'e[\-\s]*mail\s*[^\s]+@[^\s]+',
        r'www\.[^\s]+',
        r'http[s]?://[^\s]+',
        r'compartilhar\s*vaga',
        r'denunciar\s*vaga',
        r'salvar\s*vaga',
        r'favoritar\s*vaga',
    ]
    
    for pattern in noise_patterns:
        desc = re.sub(pattern, '', desc, flags=re.IGNORECASE)
    
    # Remove mÃºltiplas quebras de linha e espaÃ§os
    desc = re.sub(r'\n\s*\n', '\n', desc)
    desc = re.sub(r'\s+', ' ', desc)
    
    return desc.strip()

def extract_location_advanced(description):
    """Extrai localizaÃ§Ã£o da descriÃ§Ã£o com fallback seguro"""
    if not description or pd.isna(description):
        return None
    
    desc = str(description)
    
    # PadrÃµes para extrair localizaÃ§Ã£o
    location_patterns = [
        r'localiza[Ã§c][Ã£a]o[:\s]*([^,\n]+)',
        r'local[:\s]*([^,\n]+)',
        r'cidade[:\s]*([^,\n]+)',
        r'regi[Ã£a]o[:\s]*([^,\n]+)',
        r'endere[Ã§c]o[:\s]*([^,\n]+)',
        r'vaga\s*em\s*([^,\n]+)',
        r'oportunidade\s*em\s*([^,\n]+)',
        r'trabalhar\s*em\s*([^,\n]+)',
        # Estados brasileiros
        r'\b(acre|alagoas|amap[Ã¡a]|amazonas|bahia|cear[Ã¡a]|distrito federal|esp[Ã­i]rito santo|goi[Ã¡a]s|maranh[Ã£a]o|mato grosso|mato grosso do sul|minas gerais|par[Ã¡a]|para[Ã­i]ba|paran[Ã¡a]|pernambuco|piau[Ã­i]|rio de janeiro|rio grande do norte|rio grande do sul|rond[Ã´o]nia|roraima|santa catarina|s[Ã£a]o paulo|sergipe|tocantins)\b',
        # Siglas de estados
        r'\b(AC|AL|AP|AM|BA|CE|DF|ES|GO|MA|MT|MS|MG|PA|PB|PR|PE|PI|RJ|RN|RS|RO|RR|SC|SP|SE|TO)\b',
        # Capitais principais
        r'\b(s[Ã£a]o paulo|rio de janeiro|belo horizonte|salvador|bras[Ã­i]lia|fortaleza|manaus|curitiba|recife|porto alegre|bel[Ã©e]m|goi[Ã¢a]nia|guarulhos|campinas|s[Ã£a]o lu[Ã­i]s|s[Ã£a]o gon[Ã§c]alo|maca[eÃ©]i[Ã³o]|duque de caxias|natal|teresina|campo grande|nova igua[Ã§c]u|s[Ã£a]o bernardo do campo|jo[Ã£a]o pessoa|santos|osasco|santo andr[Ã©e]|ribeir[Ã£a]o preto|uberl[Ã¢a]ndia|sorocaba|contagem|aracaju|feira de santana|cuiab[Ã¡a]|joinville|juiz de fora|londrina|aparecida de goi[Ã¢a]nia|ananindeua|porto velho|serra|niter[Ã³o]i|caxias do sul|mau[Ã¡a]|s[Ã£a]o jo[Ã£a]o de meriti|campos dos goytacazes|vila velha|florianÃ³polis|mogi das cruzes|volta redonda|sÃ£o josÃ© dos campos|diadema|carapicuÃ­ba|betim|petropolis|jundiaÃ­|sumarÃ©|piracicaba|franca|itaquaquecetuba|embu das artes|cariacica|maringÃ¡|blumenau|paulista|viamÃ£o)\b',
    ]
    
    for pattern in location_patterns:
        match = re.search(pattern, desc, re.IGNORECASE)
        if match:
            location = match.group(1).strip()
            # Limpa a localizaÃ§Ã£o
            location = re.sub(r'^[:\-\s]+|[:\-\s]+$', '', location)
            if len(location) > 2 and len(location) < 100:
                return location
    
    return None

def extract_published_date(description):
    """Extrai data de publicaÃ§Ã£o da descriÃ§Ã£o"""
    if not description or pd.isna(description):
        return None
    
    desc = str(description).lower()
    
    # Data explÃ­cita (dd/mm/aaaa)
    date_patterns = [
        r'publicad[ao]\s*em[:\s]*(\d{1,2}\/\d{1,2}\/\d{4})',
        r'data[:\s]*(\d{1,2}\/\d{1,2}\/\d{4})',
        r'(\d{1,2}\/\d{1,2}\/\d{4})',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, desc)
        if match:
            try:
                date_str = match.group(1)
                return datetime.strptime(date_str, '%d/%m/%Y').date().isoformat()
            except ValueError:
                continue
    
    # Data relativa
    relative_patterns = [
        r'h[Ã¡a]\s*(\d+)\s*dias?',
        r'(\d+)\s*dias?\s*atr[Ã¡a]s',
        r'h[Ã¡a]\s*(\d+)\s*horas?',
        r'(\d+)\s*horas?\s*atr[Ã¡a]s',
        r'h[Ã¡a]\s*(\d+)\s*semanas?',
        r'(\d+)\s*semanas?\s*atr[Ã¡a]s',
        r'ontem',
        r'hoje',
        r'esta\s*semana',
        r'semana\s*passada',
    ]
    
    now = datetime.now()
    
    for pattern in relative_patterns:
        if pattern in ['ontem', 'hoje', r'esta\s*semana', r'semana\s*passada']:
            if re.search(pattern, desc):
                if 'hoje' in pattern:
                    return now.date().isoformat()
                elif 'ontem' in pattern:
                    return (now - timedelta(days=1)).date().isoformat()
                elif r'esta\s*semana' in pattern:
                    return now.date().isoformat()
                elif r'semana\s*passada' in pattern:
                    return (now - timedelta(days=7)).date().isoformat()
        else:
            match = re.search(pattern, desc)
            if match:
                try:
                    value = int(match.group(1))
                    if 'dia' in pattern:
                        return (now - timedelta(days=value)).date().isoformat()
                    elif 'hora' in pattern:
                        return (now - timedelta(hours=value)).date().isoformat()
                    elif 'semana' in pattern:
                        return (now - timedelta(weeks=value)).date().isoformat()
                except (ValueError, IndexError):
                    continue
    
    return None

def extract_contract_type(description):
    """Extrai tipo de contrato e modelo de trabalho"""
    if not description or pd.isna(description):
        return {"contract_type": None, "work_model": None}
    
    desc = str(description).lower()
    
    # Tipo de contrato
    contract_type = None
    contract_patterns = {
        'clt': [r'\bclt\b', r'carteira\s*assinada', r'registro\s*em\s*carteira'],
        'pj': [r'\bpj\b', r'pessoa\s*jur[Ã­i]dica', r'cnpj'],
        'estÃ¡gio': [r'est[Ã¡a]gio', r'estagi[Ã¡a]rio'],
        'temporÃ¡rio': [r'tempor[Ã¡a]rio', r'contrato\s*tempor[Ã¡a]rio'],
        'aprendiz': [r'aprendiz', r'jovem\s*aprendiz'],
        'freelancer': [r'freelancer', r'free\s*lancer', r'aut[Ã´o]nomo'],
        'terceirizado': [r'terceirizado', r'terceiriza[Ã§c][Ã£a]o'],
    }
    
    for contract, patterns in contract_patterns.items():
        for pattern in patterns:
            if re.search(pattern, desc):
                contract_type = contract
                break
        if contract_type:
            break
    
    # Modelo de trabalho
    work_model = None
    work_patterns = {
        'remoto': [r'remoto', r'home\s*office', r'trabalho\s*em\s*casa', r'distÃ¢ncia'],
        'presencial': [r'presencial', r'no\s*local', r'escritÃ³rio'],
        'hÃ­brido': [r'h[Ã­i]brido', r'misto', r'semi\s*presencial'],
    }
    
    for model, patterns in work_patterns.items():
        for pattern in patterns:
            if re.search(pattern, desc):
                work_model = model
                break
        if work_model:
            break
    
    return {"contract_type": contract_type, "work_model": work_model}

def normalize_text(text):
    """Normaliza texto removendo acentos e caracteres especiais"""
    if not text:
        return ""
    # Remove acentos
    text = unicodedata.normalize('NFD', text)
    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
    return text.lower().strip()

def extract_salary_advanced(description):
    """Extrai informaÃ§Ãµes salariais com suporte a vÃ­rgula decimal (pt-BR), faixas, perÃ­odo e comissÃ£o."""
    if not description or pd.isna(description):
        return {"min_salary": None, "max_salary": None, "salary_type": "a combinar", "commission": None, "currency": "BRL", "period": "mensal"}

    desc = str(description)
    desc_lower = desc.lower()

    # ComissÃ£o
    commission = None
    for pat in [
        r"comiss[Ã£a]o\s*de\s*([\d.,]+)%",
        r"comiss[Ã£a]o[:\s]*([\d.,]+)%",
        r"([\d.,]+)%\s*de\s*comiss[Ã£a]o",
    ]:
        m = re.search(pat, desc_lower)
        if m:
            # Converter percentual para nÃºmero
            try:
                commission = float(str(m.group(1)).replace('.', '').replace(',', '.'))
            except ValueError:
                commission = m.group(1)
            break
    if commission is None and re.search(r"\bcomiss[Ã£a]o\b", desc_lower):
        commission = "sim"

    # PerÃ­odo do salÃ¡rio
    period = "mensal"
    if re.search(r"por\s*hora|hora\b", desc_lower):
        period = "hora"
    elif re.search(r"por\s*semana|semanal", desc_lower):
        period = "semanal"
    elif re.search(r"por\s*dia|di[Ã¡a]rio", desc_lower):
        period = "diario"

    # PadrÃµes de faixa e valores Ãºnicos com formatos BR
    range_patterns = [
        r"(?:de|entre)\s*r?\$?\s*([\d\.\,]+)\s*(?:a|e|at[eÃ©])\s*r?\$?\s*([\d\.\,]+)",
        r"([\d\.\,]+)\s*(?:a|\-)\s*r?\$?\s*([\d\.\,]+)",
    ]
    single_patterns = [
        r"r?\$\s*([\d\.\,]+)",
        r"(?:sal[Ã¡a]rio|remunera[cÃ§][Ã£a]o|vencimento|ganho|renda|valor)[:\s]*r?\$?\s*([\d\.\,]+)",
    ]

    def parse_brl_number_local(s: str):
        if not s:
            return None
        s = s.strip()
        if "," in s and "." in s:
            s = s.replace(".", "").replace(",", ".")
        elif "," in s and "." not in s:
            m = re.match(r"^\d{1,3},\d{3}$", s)
            if m:
                s = s.replace(",", "")
            else:
                s = s.replace(",", ".")
        s = re.sub(r"[^0-9\.\-]", "", s)
        try:
            return float(s)
        except ValueError:
            return None

    min_salary = max_salary = None

    # Faixa salarial
    for pat in range_patterns:
        m = re.search(pat, desc_lower, flags=re.IGNORECASE)
        if m:
            v1 = parse_brl_number_local(m.group(1))
            v2 = parse_brl_number_local(m.group(2))
            if v1 is not None and v2 is not None:
                min_salary, max_salary = sorted([v1, v2])
            break

    # Valor Ãºnico se nÃ£o achou faixa
    if min_salary is None:
        for pat in single_patterns:
            m = re.search(pat, desc_lower, flags=re.IGNORECASE)
            if m:
                v = parse_brl_number_local(m.group(1))
                if v is not None:
                    min_salary = v
                break

    salary_type = "a combinar"
    if min_salary is not None and max_salary is not None:
        salary_type = "faixa salarial"
    elif min_salary is not None:
        salary_type = "valor mÃ­nimo"
    elif re.search(r"a\s*combinar|compat[Ã­i]vel|negoci[Ã¡a]vel", desc_lower):
        salary_type = "a combinar"

    return {
        "min_salary": min_salary,
        "max_salary": max_salary,
        "salary_type": salary_type,
        "commission": commission,
        "currency": "BRL",
        "period": period,
    }

def extract_experience_advanced(description):
    """Extrai informaÃ§Ãµes de experiÃªncia com padrÃµes mais detalhados"""
    if not description or pd.isna(description):
        return None
    
    desc_lower = str(description).lower()
    
    # PadrÃµes para experiÃªncia
    experience_patterns = [
        # Tempo especÃ­fico
        r'(\d+)\s*anos?\s*de\s*experi[Ãªe]ncia',
        r'experi[Ãªe]ncia\s*de\s*(\d+)\s*anos?',
        r'(\d+)\s*anos?\s*na\s*[Ã¡a]rea',
        r'(\d+)\s*anos?\s*na\s*fun[Ã§c][Ã£a]o',
        r'(\d+)\s*anos?\s*em\s*vendas',
        r'(\d+)\s*anos?\s*como',
        r'm[Ã­i]nimo\s*(\d+)\s*anos?',
        r'pelo\s*menos\s*(\d+)\s*anos?',
        # ExperiÃªncia geral
        r'experi[Ãªe]ncia\s*em\s*([^.,;]+)',
        r'experi[Ãªe]ncia\s*na\s*[Ã¡a]rea\s*de\s*([^.,;]+)',
        r'experi[Ãªe]ncia\s*como\s*([^.,;]+)',
        r'experi[Ãªe]ncia\s*pr[Ã©e]via\s*em\s*([^.,;]+)',
        r'conhecimento\s*em\s*([^.,;]+)',
        r'vivÃªncia\s*em\s*([^.,;]+)',
        r'atua[Ã§c][Ã£a]o\s*em\s*([^.,;]+)',
        # NÃ­veis de experiÃªncia
        r'experi[Ãªe]ncia\s*(iniciante|jÃºnior|pleno|sÃªnior|senior)',
        r'n[Ã­i]vel\s*(iniciante|jÃºnior|pleno|sÃªnior|senior)',
        r'profissional\s*(iniciante|jÃºnior|pleno|sÃªnior|senior)',
        # Sem experiÃªncia
        r'sem\s*experi[Ãªe]ncia',
        r'n[Ã£a]o\s*[Ã©e]\s*necess[Ã¡a]ria\s*experi[Ãªe]ncia',
        r'primeiro\s*emprego',
        r'aceita\s*iniciantes',
    ]
    
    for pattern in experience_patterns:
        match = re.search(pattern, desc_lower)
        if match:
            if match.groups():
                return match.group(1).strip()
            else:
                return "ExperiÃªncia requerida"
    
    return None

def extract_responsibilities_advanced(description):
    """Extrai responsabilidades com anÃ¡lise mais robusta e filtro de ruÃ­dos"""
    if not description or pd.isna(description):
        return []

    text = str(description)

    # NormalizaÃ§Ã£o simples
    text_norm = re.sub(r"[\t\r]+", "\n", text)

    responsibilities = []

    # PadrÃµes para identificar seÃ§Ãµes de responsabilidades (captura bloco apÃ³s cabeÃ§alhos)
    responsibility_sections = [
        r"responsabilidades?\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"atividades?\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"fun[Ã§c][Ãµo]es?\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"atribui[Ã§c][Ãµo]es?\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"principais\s*atividades\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"o\s*que\s*voc[Ãªe]\s*far[Ã¡a]\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
        r"suas\s*fun[Ã§c][Ãµo]es\s*:?[\s\-]*([\s\S]+?)(?:\n\s*\n|\n\s*(?:requisitos|qualifica[Ã§c][Ã£a]o|benef[Ã­i]cios|sal[Ã¡a]rio)\b|$)",
    ]

    # Verbos de aÃ§Ã£o para fallback (captura trechos objetivos)
    action_patterns = [
        r"(?:desenvolver|executar|realizar|coordenar|gerenciar|supervisionar|administrar|controlar|organizar|planejar|implementar|monitorar|acompanhar|elaborar|preparar|conduzir|liderar|orientar|apoiar|auxiliar|assessorar|atender|prestar|fornecer|garantir|assegurar|manter|zelar|cuidar|operar|monitorar|configurar)\s+([^\n.;]+)",
        r"(?:ser[Ã¡a]\s+respons[Ã¡a]vel\s+por|respons[Ã¡a]vel\s+por)\s+([^\n.;]+)",
        r"(?:dever[Ã¡a]|deve|ir[Ã¡a]|precisa|necess[Ã¡a]rio|obrigat[Ã³o]rio)\s+([^\n.;]+)",
        r"[â€¢\-*\u2022]\s*([^\n]+)",
        r"\d+[.)]\s*([^\n]+)",
    ]

    # Palavras de ruÃ­do para filtrar itens que nÃ£o sÃ£o responsabilidades
    noise_keywords = [
        # RemuneraÃ§Ã£o/benefÃ­cios
        r"r\$\s*\d", r"sal[Ã¡a]ri", r"remunera", r"benef[Ã­i]cio", r"comiss", r"plr", r"fgts", r"13[Âºo]", r"vale\s*(?:transporte|refei[Ã§c][Ã£a]o|alimenta[Ã§c][Ã£a]o)", r"\bvt\b", r"\bvr\b", r"\bva\b",
        # CTA e navegaÃ§Ã£o
        r"candidatar", r"candidatura", r"enviar\s+curr[iÃ­]culo", r"curr[iÃ­]culo", r"email|e\-?mail|@|http|www|clique|link|contin(uar|ue)\s+lendo",
        # Metadados e outros
        r"vaga\s+(?:publicada|dispon[iÃ­]vel)", r"somos|empresa|sobre\s+a\s+empresa",
        # Requisitos/qualificaÃ§Ãµes
        r"requisitos?|qualifica[Ã§c][Ã£a]o|experi[Ãªe]ncia\s+(?:m[Ã­i]nima|necess[Ã¡a]ria|obrigat[Ã³o]ria)", r"forma[Ã§c][Ã£a]o|escolaridade|habilidades|conhecimentos|certifica[Ã§c][Ã£a]o",
    ]
    noise_regex = re.compile(r"(" + r"|".join(noise_keywords) + r")", flags=re.IGNORECASE)

    # FunÃ§Ã£o auxiliar: quebra e limpa uma seÃ§Ã£o em itens
    def split_section_to_items(section_text: str):
        items = []
        # Divide por linhas e marcadores
        raw_lines = re.split(r"[\n\r]+|[â€¢\-*\u2022]+|\d+[.)]\s*", section_text)
        for line in raw_lines:
            line = re.sub(r"^[â€¢\-*\u2022\d.)\s]+", "", line).strip()
            # Normaliza espaÃ§os
            line = re.sub(r"\s+", " ", line)
            if len(line) < 12 or len(line) > 220:
                continue
            if noise_regex.search(line):
                continue
            # Evita frases muito genÃ©ricas
            if re.fullmatch(r"respons[Ã¡a]vel\s+por", line, flags=re.IGNORECASE):
                continue
            items.append(line)
        return items

    # 1) Busca por blocos de seÃ§Ãµes nomeadas
    for pat in responsibility_sections:
        m = re.search(pat, text_norm, flags=re.IGNORECASE)
        if m:
            section = m.group(1)
            responsibilities.extend(split_section_to_items(section))

    # 2) Se vazio, usa fallback com verbos de aÃ§Ã£o e bullets em todo o texto
    if not responsibilities:
        for pat in action_patterns:
            for m in re.findall(pat, text_norm, flags=re.IGNORECASE | re.MULTILINE):
                candidate = m if isinstance(m, str) else m[0]
                candidate = re.sub(r"^[â€¢\-*\u2022\d.)\s]+", "", candidate).strip()
                candidate = re.sub(r"\s+", " ", candidate)
                if len(candidate) < 12 or len(candidate) > 220:
                    continue
                if noise_regex.search(candidate):
                    continue
                responsibilities.append(candidate)

    # 3) PÃ³s-processamento: dedup, corte e retorno
    seen = set()
    cleaned = []
    for item in responsibilities:
        # Remove pontuaÃ§Ã£o final redundante
        item_norm = item.rstrip(" ;-.,")
        if item_norm.lower() not in seen:
            seen.add(item_norm.lower())
            cleaned.append(item_norm)

    return cleaned[:12]  # Limita a 12 responsabilidades

def extract_benefits_advanced(description):
    """Extrai benefÃ­cios com padrÃµes mais abrangentes"""
    if not description or pd.isna(description):
        return []
    
    desc_lower = str(description).lower()
    benefits = []
    
    # DicionÃ¡rio de benefÃ­cios com variaÃ§Ãµes
    benefit_patterns = {
        'Vale AlimentaÃ§Ã£o': [r'vale[\s-]*alimenta[Ã§c][Ã£a]o', r'va[\s]*alimenta[Ã§c][Ã£a]o', r'cart[Ã£a]o[\s]*alimenta[Ã§c][Ã£a]o'],
        'Vale Transporte': [r'vale[\s-]*transporte', r'vt[\s]*transporte', r'cart[Ã£a]o[\s]*transporte'],
        'Vale RefeiÃ§Ã£o': [r'vale[\s-]*refei[Ã§c][Ã£a]o', r'vr[\s]*refei[Ã§c][Ã£a]o', r'cart[Ã£a]o[\s]*refei[Ã§c][Ã£a]o'],
        'Plano de SaÃºde': [r'plano[\s]*de[\s]*sa[Ãºu]de', r'conv[Ãªe]nio[\s]*m[Ã©e]dico', r'assist[Ãªe]ncia[\s]*m[Ã©e]dica'],
        'Plano OdontolÃ³gico': [r'plano[\s]*odontol[Ã³o]gico', r'conv[Ãªe]nio[\s]*odontol[Ã³o]gico', r'assist[Ãªe]ncia[\s]*odontol[Ã³o]gica'],
        'Seguro de Vida': [r'seguro[\s]*de[\s]*vida', r'seguro[\s]*vida'],
        'ParticipaÃ§Ã£o nos Lucros': [r'participa[Ã§c][Ã£a]o[\s]*nos[\s]*lucros', r'plr', r'participa[Ã§c][Ã£a]o[\s]*lucros'],
        'DÃ©cimo Terceiro': [r'd[Ã©e]cimo[\s]*terceiro', r'13[Âºo][\s]*sal[Ã¡a]rio', r'gratifica[Ã§c][Ã£a]o[\s]*natalina'],
        'FÃ©rias': [r'f[Ã©e]rias[\s]*remuneradas', r'f[Ã©e]rias'],
        'FGTS': [r'fgts', r'fundo[\s]*de[\s]*garantia'],
        'PIS': [r'pis', r'programa[\s]*de[\s]*integra[Ã§c][Ã£a]o[\s]*social'],
        'ComissÃ£o': [r'comiss[Ã£a]o', r'comissionamento'],
        'Home Office': [r'home[\s]*office', r'trabalho[\s]*remoto', r'trabalho[\s]*em[\s]*casa'],
        'HorÃ¡rio FlexÃ­vel': [r'hor[Ã¡a]rio[\s]*flex[Ã­i]vel', r'flexibilidade[\s]*de[\s]*hor[Ã¡a]rio'],
        'Treinamento': [r'treinamento', r'capacita[Ã§c][Ã£a]o', r'curso[\s]*de[\s]*forma[Ã§c][Ã£a]o'],
        'Estacionamento': [r'estacionamento', r'vaga[\s]*de[\s]*estacionamento'],
        'Cesta BÃ¡sica': [r'cesta[\s]*b[Ã¡a]sica', r'cesta[\s]*de[\s]*alimentos'],
        'AuxÃ­lio Creche': [r'aux[Ã­i]lio[\s]*creche', r'aux[Ã­i]lio[\s]*bab[Ã¡a]'],
        'AuxÃ­lio EducaÃ§Ã£o': [r'aux[Ã­i]lio[\s]*educa[Ã§c][Ã£a]o', r'aux[Ã­i]lio[\s]*estudo'],
        'GinÃ¡stica Laboral': [r'gin[Ã¡a]stica[\s]*laboral', r'atividade[\s]*f[Ã­i]sica'],
        'RefeitÃ³rio': [r'refeit[Ã³o]rio', r'restaurante[\s]*da[\s]*empresa'],
        'AlmoÃ§o': [r'almo[Ã§c]o[\s]*gratuito', r'almo[Ã§c]o[\s]*fornecido', r'almo[Ã§c]o'],
        'Lanche': [r'lanche[\s]*gratuito', r'caf[Ã©e][\s]*da[\s]*manh[Ã£a]', r'lanche'],
        'Uniforme': [r'uniforme[\s]*fornecido', r'uniforme'],
        'Equipamentos': [r'equipamentos[\s]*fornecidos', r'ferramentas[\s]*de[\s]*trabalho'],
    }
    
    # Busca por cada benefÃ­cio
    for benefit_name, patterns in benefit_patterns.items():
        for pattern in patterns:
            if re.search(pattern, desc_lower):
                benefits.append(benefit_name)
                break
    
    return list(set(benefits))  # Remove duplicatas

def extract_education_and_skills(description):
    """Extrai formaÃ§Ã£o acadÃªmica e habilidades da descriÃ§Ã£o"""
    if not description or pd.isna(description):
        return {"education": [], "skills": []}
    
    desc = str(description)
    desc_lower = desc.lower()
    education = []
    skills = set()
    
    # PadrÃµes de formaÃ§Ã£o acadÃªmica (pt-BR)
    edu_patterns = [
        r'(ensino\s+m[Ã©e]dio\s+(?:completo|incompleto))',
        r'(ensino\s+t[Ã©e]cnico\s+(?:completo|incompleto))',
        r'(ensino\s+superior\s+(?:completo|incompleto))',
        r'(t[Ã©e]cnico\s+em\s+[\wÃ€-Ã¿\s]+)',
        r'(gradua[Ã§c][Ã£a]o\s+em\s+[\wÃ€-Ã¿\s]+)',
        r'(forma[Ã§c][Ã£a]o\s+em\s+[\wÃ€-Ã¿\s]+)',
        r'(p[Ã³o]s[-\s]*gradua[Ã§c][Ã£a]o)',
        r'(especializa[Ã§c][Ã£a]o)',
        r'(mba)',
        r'(mestrado)',
        r'(doutorado)'
    ]
    for pat in edu_patterns:
        for m in re.findall(pat, desc_lower, flags=re.IGNORECASE):
            val = m if isinstance(m, str) else m[0]
            val = val.strip()
            if val and val not in education:
                education.append(val)
    
    # PadrÃµes de habilidades (hard/soft e ferramentas)
    skill_map = {
        'excel': [r'\bexcel\b', r'pacote\s*office'],
        'word': [r'\bword\b'],
        'power bi': [r'power\s*bi'],
        'sql': [r'\bsql\b'],
        'python': [r'\bpython\b'],
        'java': [r'\bjava\b'],
        'javascript': [r'javascript', r'\bjs\b', r'node\.?js', r'react'],
        'c#': [r'\bc#\b', r'csharp'],
        'git': [r'\bgit\b', r'github', r'gitlab'],
        'linux': [r'linux'],
        'comunicaÃ§Ã£o': [r'comunica[Ã§c][Ã£a]o'],
        'lideranÃ§a': [r'lideran[Ã§c]a'],
        'negociaÃ§Ã£o': [r'negocia[Ã§c][Ã£a]o'],
        'vendas': [r'\bvendas\b'],
        'atendimento ao cliente': [r'atendimento\s+ao\s+cliente'],
        'crm': [r'\bcrm\b'],
        'erp': [r'\berp\b', r'sap'],
        'autocad': [r'autocad'],
        'solidworks': [r'solid\s*works'],
        'marketing digital': [r'marketing\s*digital'],
        'redes sociais': [r'redes\s*sociais'],
        'photoshop': [r'photoshop'],
        'illustrator': [r'illustrator'],
        'powerpoint': [r'power\s*point', r'powerpoint'],
        'inglÃªs': [r'\bingl[Ãªe]s\b'],
        'espanhol': [r'\bespanhol\b']
    }
    for canonical, patterns in skill_map.items():
        for pat in patterns:
            if re.search(pat, desc_lower, flags=re.IGNORECASE):
                skills.add(canonical)
                break
    
    # SeÃ§Ãµes de requisitos podem conter habilidades adicionais (captura genÃ©rica de bullets apÃ³s "Requisitos")
    req_sections = [
        r'requisitos?[:\s]*([\s\S]{0,400})',
        r'qualifica[Ã§c][Ãµo]es?[:\s]*([\s\S]{0,400})',
        r'conhecimentos?[:\s]*([\s\S]{0,400})'
    ]
    for pat in req_sections:
        m = re.search(pat, desc, flags=re.IGNORECASE)
        if m:
            snippet = m.group(1)
            for canonical, patterns in skill_map.items():
                for sp in patterns:
                    if re.search(sp, snippet, flags=re.IGNORECASE):
                        skills.add(canonical)
                        break
    
    return {"education": education[:5], "skills": sorted(list(skills))[:15]}

def calculate_data_quality_score(job):
    """Calcula score de qualidade dos dados extraÃ­dos"""
    score = 0
    
    # SalÃ¡rio (peso 2)
    if job.get('salary', {}).get('min_salary') and job['salary']['salary_type'] != 'a combinar':
        score += 2
    
    # ExperiÃªncia (peso 1)
    if job.get('experience'):
        score += 1
    
    # Responsabilidades (peso 1)
    if job.get('responsibilities') and len(job['responsibilities']) > 0:
        score += 1
    
    # BenefÃ­cios (peso 1)
    if job.get('benefits') and len(job['benefits']) > 0:
        score += 1
    
    # ComissÃ£o (peso 0.5)
    if job.get('salary', {}).get('commission'):
        score += 0.5
    
    # FormaÃ§Ã£o (peso 0.5)
    if job.get('education'):
        score += 0.5
    
    # Habilidades (peso 0.5)
    if job.get('skills'):
        score += 0.5
    
    return score

def process_csv_master(csv_file):
    """Processa CSV com extraÃ§Ã£o master de dados"""
    print("ğŸš€ Iniciando extraÃ§Ã£o MASTER de dados...")
    
    # Carrega o CSV
    df = pd.read_csv(csv_file, encoding='utf-8')
    print(f"ğŸ“Š Total de vagas carregadas: {len(df)}")
    
    jobs_data = []
    stats = {
        'total_jobs': len(df),
        'with_salary': 0,
        'with_commission': 0,
        'with_experience': 0,
        'with_responsibilities': 0,
        'with_benefits': 0,
        'high_quality': 0
    }
    
    for index, row in df.iterrows():
        if index % 1000 == 0:
            print(f"âš¡ Processando vaga {index + 1}/{len(df)}...")
        
        # Extrai dados da descriÃ§Ã£o
        description_raw = row.get('DescriÃ§Ã£o', '')
        description = clean_description(description_raw)
        
        salary_info = extract_salary_advanced(description)
        experience = extract_experience_advanced(description)
        responsibilities = extract_responsibilities_advanced(description)
        benefits = extract_benefits_advanced(description)
        edu_skills = extract_education_and_skills(description)
        
        # Campos adicionais extraÃ­dos da descriÃ§Ã£o
        location_extracted = extract_location_advanced(description)
        published_date = extract_published_date(description)
        contract_info = extract_contract_type(description)
        
        # Fallbacks e consolidaÃ§Ã£o
        location_csv = row.get('Localidade', '')
        location_final = location_csv if (isinstance(location_csv, str) and location_csv.strip()) else location_extracted
        
        work_type_csv = row.get('Modalidade', '')
        work_type_final = work_type_csv if (isinstance(work_type_csv, str) and work_type_csv.strip()) else contract_info.get('work_model')
        
        # Cria objeto da vaga
        job = {
            'id': index + 1,
            'title': row.get('TÃ­tulo', ''),
            'company': row.get('Empresa', ''),
            'location': location_final,
            'work_type': work_type_final,
            'description': description,
            'description_raw': description_raw,
            'link': row.get('Link', ''),
            'salary': salary_info,
            'experience': experience,
            'responsibilities': responsibilities,
            'benefits': benefits,
            'published_date': published_date,
            'contract_type': contract_info.get('contract_type'),
            'work_model': contract_info.get('work_model'),
            'location_extracted': location_extracted,
            'education': edu_skills.get('education', []),
            'skills': edu_skills.get('skills', []),
            'extraction_timestamp': datetime.now().isoformat()
        }
        
        # Calcula score de qualidade
        quality_score = calculate_data_quality_score(job)
        job['data_quality_score'] = quality_score
        
        # Atualiza estatÃ­sticas
        if salary_info['min_salary'] and salary_info['salary_type'] != 'a combinar':
            stats['with_salary'] += 1
        if salary_info['commission']:
            stats['with_commission'] += 1
        if experience:
            stats['with_experience'] += 1
        if responsibilities:
            stats['with_responsibilities'] += 1
        if benefits:
            stats['with_benefits'] += 1
        if quality_score >= 3:
            stats['high_quality'] += 1
        
        jobs_data.append(job)
    
    # Cria estrutura final
    final_data = {
        'metadata': {
            'extraction_date': datetime.now().isoformat(),
            'total_jobs': stats['total_jobs'],
            'extraction_stats': stats,
            'extraction_method': 'master_advanced_extraction',
            'version': '1.0'
        },
        'jobs': jobs_data
    }
    
    # Salva o arquivo JSON
    output_file = 'catho_worker3_master_extraction.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ExtraÃ§Ã£o MASTER concluÃ­da!")
    print(f"ğŸ“ Arquivo gerado: {output_file}")
    print(f"ğŸ“Š EstatÃ­sticas:")
    print(f"   ğŸ’° SalÃ¡rios extraÃ­dos: {stats['with_salary']} ({stats['with_salary']/stats['total_jobs']*100:.1f}%)")
    print(f"   ğŸ’¸ ComissÃµes extraÃ­das: {stats['with_commission']} ({stats['with_commission']/stats['total_jobs']*100:.1f}%)")
    print(f"   â° ExperiÃªncia extraÃ­da: {stats['with_experience']} ({stats['with_experience']/stats['total_jobs']*100:.1f}%)")
    print(f"   ğŸ“‹ Responsabilidades: {stats['with_responsibilities']} ({stats['with_responsibilities']/stats['total_jobs']*100:.1f}%)")
    print(f"   ğŸ BenefÃ­cios: {stats['with_benefits']} ({stats['with_benefits']/stats['total_jobs']*100:.1f}%)")
    print(f"   ğŸŒŸ Alta qualidade: {stats['high_quality']} ({stats['high_quality']/stats['total_jobs']*100:.1f}%)")
    
    return output_file

if __name__ == "__main__":
    # Processa o arquivo CSV
    csv_file = 'catho_batch_009_vagas_36001-40500.csv'
    process_csv_master(csv_file)